from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from Extract.weather_extractor import get_weather_for_south_regions
from Extract.fao_extractor import get_fao_data
from Extract.copernicus_extractor import get_copernicus_data
from Transform.transform_data import transform
from Load.load_to_postgres import load_to_postgres
import pandas as pd
import os
import pathlib
import gc
import logging

# Configuration logging
logger = logging.getLogger(__name__)


def log_memory_usage(step_name):
    """Log l'utilisation mÃ©moire actuelle"""
    import psutil
    process = psutil.Process()
    mem_info = process.memory_info()
    mem_mb = mem_info.rss / 1024 / 1024
    logger.info(f"ğŸ’¾ [{step_name}] MÃ©moire utilisÃ©e: {mem_mb:.2f} MB")


def extract_data(**context):
    """Extraction optimisÃ©e avec gestion mÃ©moire"""
    logger.info("ğŸš€ DÃ©but extraction des donnÃ©es...")
    log_memory_usage("DÃ©but")
    
    # Configuration du rÃ©pertoire de travail
    project_root = pathlib.Path(__file__).resolve().parents[1]
    data_candidate = project_root / "data_corpinius" / "swi_global_12.5km_10daily_v3_cog.csv"
    alt_candidate = pathlib.Path("/opt/airflow") / "data_corpinius" / "swi_global_12.5km_10daily_v3_cog.csv"
    
    if data_candidate.exists():
        os.chdir(project_root)
        logger.info(f"ğŸ“ CWD changÃ© vers {project_root}")
    elif alt_candidate.exists():
        os.chdir(alt_candidate.parents[1])
        logger.info(f"ğŸ“ CWD changÃ© vers {alt_candidate.parents[1]}")
    else:
        raise FileNotFoundError(
            f"Fichier de donnÃ©es introuvable.\n"
            f"VÃ©rifiÃ© : {data_candidate}\n"
            f"VÃ©rifiÃ© : {alt_candidate}"
        )
    
    # CrÃ©er le dossier de sortie
    os.makedirs("/tmp/data", exist_ok=True)
    
    # âš¡ EXTRACTION 1 : WEATHER (traiter et libÃ©rer immÃ©diatement)
    try:
        logger.info("ğŸ“¡ Extraction Weather...")
        weather = get_weather_for_south_regions()
        logger.info(f"âœ… Weather rÃ©cupÃ©rÃ©: {len(weather)} lignes")
        log_memory_usage("AprÃ¨s Weather")
        
        # Sauvegarder IMMÃ‰DIATEMENT
        weather.to_csv("/tmp/data/weather.csv", index=False)
        
        # LibÃ©rer la mÃ©moire
        del weather
        gc.collect()
        log_memory_usage("AprÃ¨s nettoyage Weather")
        
    except Exception as e:
        logger.error(f"âŒ Erreur Weather: {str(e)}")
        raise
    
    # âš¡ EXTRACTION 2 : FAO (traiter et libÃ©rer immÃ©diatement)
    try:
        logger.info("ğŸ“¡ Extraction FAO...")
        
        # âœ… Nouveau (simplifiÃ©)
        fao = get_fao_data()
        logger.info(f"âœ… FAO rÃ©cupÃ©rÃ©: {len(fao)} lignes")
        log_memory_usage("AprÃ¨s FAO")
        
        # Sauvegarder IMMÃ‰DIATEMENT
        fao.to_csv("/tmp/data/fao.csv", index=False)
        
        # LibÃ©rer la mÃ©moire
        del fao
        gc.collect()
        log_memory_usage("AprÃ¨s nettoyage FAO")
        
    except Exception as e:
        logger.error(f"âŒ Erreur FAO: {str(e)}")
        raise
    
    # âš¡ EXTRACTION 3 : COPERNICUS (le plus lourd - optimisÃ©)
    try:
        logger.info("ğŸ“¡ Extraction Copernicus...")
        
        # DÃ©sactiver temporairement le GC pour performance
        gc.disable()
        
        copernicus = get_copernicus_data()
        logger.info(f"âœ… Copernicus rÃ©cupÃ©rÃ©: {len(copernicus)} lignes")
        log_memory_usage("AprÃ¨s Copernicus")
        
        # Sauvegarder par morceaux si le DataFrame est trÃ¨s grand
        if len(copernicus) > 100000:
            logger.info("âš ï¸ Grand dataset dÃ©tectÃ©, Ã©criture par morceaux...")
            chunk_size = 50000
            for i in range(0, len(copernicus), chunk_size):
                chunk = copernicus.iloc[i:i+chunk_size]
                mode = 'w' if i == 0 else 'a'
                header = i == 0
                chunk.to_csv("/tmp/data/copernicus.csv", mode=mode, header=header, index=False)
                logger.info(f"âœï¸ Chunk {i//chunk_size + 1} Ã©crit")
        else:
            copernicus.to_csv("/tmp/data/copernicus.csv", index=False)
        
        # LibÃ©rer la mÃ©moire
        del copernicus
        
        # RÃ©activer et forcer le GC
        gc.enable()
        gc.collect()
        log_memory_usage("AprÃ¨s nettoyage Copernicus")
        
    except Exception as e:
        gc.enable()  # S'assurer que le GC est rÃ©activÃ© mÃªme en cas d'erreur
        logger.error(f"âŒ Erreur Copernicus: {str(e)}")
        raise
    
    logger.info("âœ… Extraction terminÃ©e avec succÃ¨s")
    log_memory_usage("Fin extraction")


def transform_data(**context):
    logger.info("ğŸ”„ DÃ©but transformation...")
    log_memory_usage("DÃ©but transform")
    
    try:
        logger.info("ğŸ“¥ Chargement weather...")
        weather = pd.read_csv("/tmp/data/weather.csv", low_memory=False)
        log_memory_usage("AprÃ¨s chargement weather")
        
        logger.info("ğŸ“¥ Chargement FAO...")
        fao = pd.read_csv("/tmp/data/fao.csv", low_memory=False)
        log_memory_usage("AprÃ¨s chargement FAO")
        
        # âš¡ Suppression du chargement Copernicus
        # logger.info("ğŸ“¥ Chargement Copernicus...")
        # copernicus = pd.read_csv("/tmp/data/copernicus.csv", low_memory=False)
        # log_memory_usage("AprÃ¨s chargement Copernicus")
        
        logger.info("âš™ï¸ Application des transformations...")
        df_final = transform(weather, fao)  # On passe seulement weather + fao
        
        del weather, fao
        gc.collect()
        log_memory_usage("AprÃ¨s transformation")
        
        df_final.to_csv("/tmp/data/final.csv", index=False)
        logger.info(f"âœ… Transformation terminÃ©e: {len(df_final)} lignes finales")
        
        del df_final
        gc.collect()
        log_memory_usage("Fin transform")
        
    except Exception as e:
        logger.error(f"âŒ Erreur transformation: {str(e)}")
        raise


def load_data(**context):
    """Chargement optimisÃ© vers PostgreSQL"""
    logger.info("ğŸ“¤ DÃ©but chargement PostgreSQL...")
    log_memory_usage("DÃ©but load")
    
    try:
        # Charger par morceaux si nÃ©cessaire
        chunk_size = 10000
        first_chunk = True
        
        for chunk in pd.read_csv("/tmp/data/final.csv", chunksize=chunk_size):
            logger.info(f"ğŸ“¦ Chargement chunk de {len(chunk)} lignes...")
            # âœ… CORRECTION : Utiliser 'chunk' au lieu de 'df'
            load_to_postgres(chunk, 'weather_agro_data', if_exists='append')
            first_chunk = False
            
            # LibÃ©rer le chunk
            del chunk
            gc.collect()
        
        logger.info("âœ… DonnÃ©es chargÃ©es dans PostgreSQL !")
        log_memory_usage("Fin load")
        
    except Exception as e:
        logger.error(f"âŒ Erreur chargement: {str(e)}")
        raise

# --- DÃ©finition du DAG ---
default_args = {
    'owner': 'hackathon_databeez',
    'retries': 2,
    'retry_delay': timedelta(minutes=3),
    'execution_timeout': timedelta(minutes=20),  # Timeout global
}

with DAG(
    'etl_hackathon_dag',
    default_args=default_args,
    description='Pipeline ETL mÃ©tÃ©o/agro optimisÃ© pour Hackathon DataBeez',
    schedule_interval=timedelta(minutes=30),
    start_date=datetime(2025, 10, 15),
    catchup=False,
    max_active_runs=1,  # Ã‰viter les exÃ©cutions simultanÃ©es
) as dag:
    
    extract_task = PythonOperator(
        task_id='extract_data',
        python_callable=extract_data,
        execution_timeout=timedelta(minutes=10),
    )
    
    transform_task = PythonOperator(
        task_id='transform_data',
        python_callable=transform_data,
        execution_timeout=timedelta(minutes=8),
    )
    
    load_task = PythonOperator(
        task_id='load_data',
        python_callable=load_data,
        execution_timeout=timedelta(minutes=5),
    )
    
    # ChaÃ®nage du pipeline
    extract_task >> transform_task >> load_task